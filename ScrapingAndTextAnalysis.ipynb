{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9a3d9a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import random\n",
    "import os\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "68bd8927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mahithaappasani/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import FreqDist\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "from string import punctuation\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e6f8bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScrapeText(url,lower_index,upper_index):\n",
    "    #we are using request package to make a GET request for the website, which means we're getting data from it.\n",
    "    r=requests.get(url)\n",
    "\n",
    "    #Setting the correct text encoding of the HTML page\n",
    "    r.encoding = 'utf-8'\n",
    "\n",
    "    #Extracting the HTML from the request object\n",
    "    html = r.text\n",
    "    \n",
    "    # Creating a BeautifulSoup object from the HTML\n",
    "    soup = BeautifulSoup(html)\n",
    "\n",
    "    # Getting the text out of the soup\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    #getting just the main portion of text\n",
    "    main_text = text[lower_index:upper_index]\n",
    "    \n",
    "    clean_text= main_text.replace(\"/n\", \" \")\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2260b61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMostCommonWords(clean_text):\n",
    "    #Splitting data into words and getting meaningful words from text\n",
    "    words = word_tokenize(clean_text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    remove_words = ['\\'s','per','  \\'']\n",
    "    stop_words.update(remove_words)\n",
    "\n",
    "    words_no_punctuation = [word for word in words if word not in punctuation]\n",
    "    meaningful_words = [word for word in words_no_punctuation if word.casefold() not in stop_words]\n",
    "    \n",
    "    #Getting the top 100 most common words\n",
    "    top_frequency_distribution = FreqDist(meaningful_words).most_common(50)\n",
    "    top_frequency_df = pd.DataFrame(top_frequency_distribution, columns =['Word', 'Frequency'])\n",
    "    \n",
    "    return top_frequency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ee0c8d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMostCommonPhrases(clean_text):\n",
    "    noun_phrase = list()\n",
    "    for chunk in nlp(clean_text).noun_chunks:\n",
    "        new = str(chunk)\n",
    "        if len(new.split()) != 1:\n",
    "            noun_phrase.append(new)\n",
    "\n",
    "    frequency_distribution_np = FreqDist(noun_phrase).most_common(30)    \n",
    "\n",
    "    noun_phrase_df = pd.DataFrame(frequency_distribution_np, columns =['Noun Phrase', 'Frequency'])\n",
    "    \n",
    "    return noun_phrase_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2e8161a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = ['https://koreanserums.com/american-beauty-standards/',\n",
    "        'https://tower.mastersny.org/10976/features/the-western-lens-on-beauty-how-european-beauty-standards-are-still-in-play-today/#:~:text=The%20features%20which%20tend%20to,decade%2C%20it%20still%20remains%20dominant.',\n",
    "           'https://aninjusticemag.com/stop-imposing-eurocentric-beauty-standards-1560f58593d2']\n",
    "\n",
    "#lower and upper indices for each article to get main text\n",
    "indices_list=[[562,11835],[1422,8155],[343,7100]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "99648403",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(url_list)):\n",
    "    url = url_list[i]\n",
    "    lower_index = indices_list[i][0]\n",
    "    upper_index = indices_list[i][1]\n",
    "    \n",
    "    cleaned_text = ScrapeText(url,lower_index,upper_index)\n",
    "    top_words_df = getMostCommonWords(cleaned_text)\n",
    "    top_phrases_df = getMostCommonPhrases(cleaned_text)\n",
    "    \n",
    "    top_words_df.to_excel('Beauty Standards Article ' + str(i+1) + ' Top Words.xlsx')\n",
    "    top_phrases_df.to_excel('Beauty Standards Article ' + str(i+1) + ' Top Phrases.xlsx')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f4fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
